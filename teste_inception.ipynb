{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Adagrad\n",
    "import tensorflow as tf\n",
    "import config\n",
    "import utils\n",
    "from model import custom_inceptionResnetV2_conv_global, custom_inceptionResnetV3_conv_global, custom_inceptionV3_attention, custom_inceptionV2_attention\n",
    "import cv2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1082\n",
      "471\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_train_images = utils.total_count_files(config.TRAIN_DIR)\n",
    "print(total_train_images)\n",
    "\n",
    "total_val_images = utils.total_count_files(config.TESTE_DIR)\n",
    "print(total_val_images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(image):\n",
    "    sigmaX = 10\n",
    "    return cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX), -4, 128)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1082 images belonging to 2 classes.\n",
      "Found 471 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_train = ImageDataGenerator(\n",
    "                                rescale=1/255,\n",
    "                                rotation_range=40,\n",
    "                                width_shift_range=0.2,\n",
    "                                height_shift_range=0.2,\n",
    "                                shear_range=0.2,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                fill_mode='nearest',\n",
    "                                preprocessing_function=gaussian_filter\n",
    "                                )\n",
    "\n",
    "train_generator = dataset_train.flow_from_directory(config.TRAIN_DIR,\n",
    "                                                    classes = [\"0\", \"1\"],\n",
    "                                                    target_size=config.image_size_gen,\n",
    "                                                    batch_size=config.batch,\n",
    "                                                    class_mode='categorical')\n",
    "\n",
    "valida_generator = dataset_train.flow_from_directory(config.TESTE_DIR,\n",
    "                                                    classes = [\"0\", \"1\"],\n",
    "                                                    target_size=config.image_size_gen,\n",
    "                                                    batch_size=config.batch,\n",
    "                                                    class_mode='categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_inception_v2 = custom_inceptionResnetV2_conv_global()\n",
    "\n",
    "#optimizer\n",
    "optimizer=SGD(learning_rate=1e-3,\n",
    "              momentum=0.9,\n",
    "              nesterov=True)\n",
    "optimizer_rms=RMSprop(learning_rate=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "optimizer_adam=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "optimizer_adagrad=Adagrad(learning_rate=0.001, epsilon=None, decay=0.0)\n",
    "    \n",
    "objective=\"categorical_crossentropy\"\n",
    "\n",
    "model_inception_v2.compile(optimizer=optimizer_adam,\n",
    "              loss=objective,\n",
    "              metrics=['categorical_accuracy']\n",
    "              )\n",
    "\n",
    "# callbacks\n",
    "history = utils.LossHistory()\n",
    "early_stopping = utils.set_early_stopping()\n",
    "reduce_lr = utils.set_reduce_lr()\n",
    "\n",
    "steps_train = int(total_train_images // config.batch)\n",
    "steps_val = int(total_val_images // config.batch)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO INCEPTION V2 SEM ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: categorical_accuracy improved from -inf to 0.74296, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 377s - loss: 0.5371 - categorical_accuracy: 0.7430 - val_loss: 0.4578 - val_categorical_accuracy: 0.7823 - lr: 0.0010 - 377s/epoch - 6s/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: categorical_accuracy improved from 0.74296 to 0.81801, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 327s - loss: 0.4057 - categorical_accuracy: 0.8180 - val_loss: 0.4137 - val_categorical_accuracy: 0.8427 - lr: 0.0010 - 327s/epoch - 5s/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: categorical_accuracy improved from 0.81801 to 0.84803, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 302s - loss: 0.3667 - categorical_accuracy: 0.8480 - val_loss: 0.3841 - val_categorical_accuracy: 0.8427 - lr: 0.0010 - 302s/epoch - 5s/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: categorical_accuracy improved from 0.84803 to 0.85553, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 302s - loss: 0.3488 - categorical_accuracy: 0.8555 - val_loss: 0.4425 - val_categorical_accuracy: 0.8297 - lr: 0.0010 - 302s/epoch - 5s/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: categorical_accuracy did not improve from 0.85553\n",
      "67/67 - 311s - loss: 0.3451 - categorical_accuracy: 0.8424 - val_loss: 0.3721 - val_categorical_accuracy: 0.8384 - lr: 0.0010 - 311s/epoch - 5s/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: categorical_accuracy improved from 0.85553 to 0.86492, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 293s - loss: 0.3206 - categorical_accuracy: 0.8649 - val_loss: 0.3874 - val_categorical_accuracy: 0.8513 - lr: 0.0010 - 293s/epoch - 4s/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: categorical_accuracy did not improve from 0.86492\n",
      "67/67 - 299s - loss: 0.3937 - categorical_accuracy: 0.8340 - val_loss: 0.3874 - val_categorical_accuracy: 0.8470 - lr: 0.0010 - 299s/epoch - 4s/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: categorical_accuracy did not improve from 0.86492\n",
      "67/67 - 322s - loss: 0.3162 - categorical_accuracy: 0.8565 - val_loss: 0.3523 - val_categorical_accuracy: 0.8578 - lr: 0.0010 - 322s/epoch - 5s/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: categorical_accuracy did not improve from 0.86492\n",
      "67/67 - 294s - loss: 0.3266 - categorical_accuracy: 0.8593 - val_loss: 0.3873 - val_categorical_accuracy: 0.8341 - lr: 0.0010 - 294s/epoch - 4s/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: categorical_accuracy did not improve from 0.86492\n",
      "67/67 - 300s - loss: 0.3222 - categorical_accuracy: 0.8565 - val_loss: 0.3442 - val_categorical_accuracy: 0.8578 - lr: 0.0010 - 300s/epoch - 4s/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: categorical_accuracy improved from 0.86492 to 0.87054, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 310s - loss: 0.3056 - categorical_accuracy: 0.8705 - val_loss: 0.3578 - val_categorical_accuracy: 0.8513 - lr: 0.0010 - 310s/epoch - 5s/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: categorical_accuracy improved from 0.87054 to 0.87805, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 291s - loss: 0.3021 - categorical_accuracy: 0.8780 - val_loss: 0.3739 - val_categorical_accuracy: 0.8297 - lr: 0.0010 - 291s/epoch - 4s/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: categorical_accuracy did not improve from 0.87805\n",
      "67/67 - 289s - loss: 0.2846 - categorical_accuracy: 0.8759 - val_loss: 0.3871 - val_categorical_accuracy: 0.8685 - lr: 0.0010 - 289s/epoch - 4s/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: categorical_accuracy did not improve from 0.87805\n",
      "67/67 - 301s - loss: 0.3003 - categorical_accuracy: 0.8659 - val_loss: 0.3357 - val_categorical_accuracy: 0.8578 - lr: 0.0010 - 301s/epoch - 4s/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: categorical_accuracy improved from 0.87805 to 0.88837, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 287s - loss: 0.2797 - categorical_accuracy: 0.8884 - val_loss: 0.3444 - val_categorical_accuracy: 0.8513 - lr: 0.0010 - 287s/epoch - 4s/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 292s - loss: 0.2804 - categorical_accuracy: 0.8780 - val_loss: 0.3793 - val_categorical_accuracy: 0.8470 - lr: 0.0010 - 292s/epoch - 4s/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 323s - loss: 0.2876 - categorical_accuracy: 0.8734 - val_loss: 0.3565 - val_categorical_accuracy: 0.8685 - lr: 0.0010 - 323s/epoch - 5s/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 317s - loss: 0.3085 - categorical_accuracy: 0.8715 - val_loss: 0.3262 - val_categorical_accuracy: 0.8599 - lr: 0.0010 - 317s/epoch - 5s/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 313s - loss: 0.2823 - categorical_accuracy: 0.8827 - val_loss: 0.3539 - val_categorical_accuracy: 0.8405 - lr: 0.0010 - 313s/epoch - 5s/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 311s - loss: 0.2626 - categorical_accuracy: 0.8874 - val_loss: 0.3201 - val_categorical_accuracy: 0.8534 - lr: 0.0010 - 311s/epoch - 5s/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 293s - loss: 0.2721 - categorical_accuracy: 0.8790 - val_loss: 0.3404 - val_categorical_accuracy: 0.8707 - lr: 0.0010 - 293s/epoch - 4s/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 295s - loss: 0.3172 - categorical_accuracy: 0.8668 - val_loss: 0.3862 - val_categorical_accuracy: 0.8427 - lr: 0.0010 - 295s/epoch - 4s/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 313s - loss: 0.2719 - categorical_accuracy: 0.8799 - val_loss: 0.3227 - val_categorical_accuracy: 0.8642 - lr: 0.0010 - 313s/epoch - 5s/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: categorical_accuracy improved from 0.88837 to 0.89024, saving model to output/inceptionv2/normal\\pesos_inception_best.h5\n",
      "67/67 - 296s - loss: 0.2666 - categorical_accuracy: 0.8902 - val_loss: 0.3685 - val_categorical_accuracy: 0.8384 - lr: 0.0010 - 296s/epoch - 4s/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: categorical_accuracy did not improve from 0.89024\n",
      "67/67 - 295s - loss: 0.2646 - categorical_accuracy: 0.8856 - val_loss: 0.3681 - val_categorical_accuracy: 0.8448 - lr: 5.0000e-04 - 295s/epoch - 4s/step\n",
      "Epoch 25: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 244). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/inceptionv2/normal/model_inception\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/inceptionv2/normal/model_inception\\assets\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'output/inceptionv2/normal/pesos_inception_best.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='categorical_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')\n",
    "\n",
    "# training model\n",
    "history = model_inception_v2.fit(train_generator,\n",
    "                            steps_per_epoch = steps_train,\n",
    "                            epochs = config.epochs,\n",
    "                            callbacks=[history, early_stopping, reduce_lr, checkpoint],\n",
    "                            validation_data = valida_generator,\n",
    "                            validation_steps = steps_val,\n",
    "                            verbose = 2)\n",
    "\n",
    "model_inception_v2.save_weights('output/inceptionv2/normal/pesos_inception.h5')\n",
    "model_inception_v2.save('output/inceptionv2/normal/model_inception')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODE INCEPTION V3 COM ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 222, 222, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " inception_v3 (Functional)      (None, 5, 5, 2048)   21802784    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_297 (Batch  (None, 5, 5, 2048)  8192        ['inception_v3[0][0]']           \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 5, 2048)   0           ['batch_normalization_297[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_297 (Conv2D)            (None, 5, 5, 64)     131136      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_298 (Conv2D)            (None, 5, 5, 16)     1040        ['conv2d_297[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_299 (Conv2D)            (None, 5, 5, 8)      136         ['conv2d_298[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_300 (Conv2D)            (None, 5, 5, 1)      9           ['conv2d_299[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_301 (Conv2D)            (None, 5, 5, 2048)   2048        ['conv2d_300[0][0]']             \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 5, 5, 2048)   0           ['conv2d_301[0][0]',             \n",
      "                                                                  'batch_normalization_297[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 2048)        0           ['multiply[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 2048)        0           ['conv2d_301[0][0]']             \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " RescaleGAP (Lambda)            (None, 2048)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_average_pooling2d_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 2048)         0           ['RescaleGAP[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          262272      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            258         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,207,875\n",
      "Trainable params: 398,947\n",
      "Non-trainable params: 21,808,928\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_inception_v3_att = custom_inceptionV3_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: categorical_accuracy improved from -inf to 0.76829, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 224s - loss: 0.5362 - categorical_accuracy: 0.7683 - top_2_accuracy: 1.0000 - val_loss: 0.3965 - val_categorical_accuracy: 0.8405 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 224s/epoch - 3s/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: categorical_accuracy improved from 0.76829 to 0.83302, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 218s - loss: 0.3821 - categorical_accuracy: 0.8330 - top_2_accuracy: 1.0000 - val_loss: 0.3593 - val_categorical_accuracy: 0.8448 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 218s/epoch - 3s/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: categorical_accuracy improved from 0.83302 to 0.84053, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 217s - loss: 0.3690 - categorical_accuracy: 0.8405 - top_2_accuracy: 1.0000 - val_loss: 0.3430 - val_categorical_accuracy: 0.8470 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 217s/epoch - 3s/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: categorical_accuracy did not improve from 0.84053\n",
      "67/67 - 235s - loss: 0.3484 - categorical_accuracy: 0.8377 - top_2_accuracy: 1.0000 - val_loss: 0.3733 - val_categorical_accuracy: 0.8362 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 235s/epoch - 4s/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: categorical_accuracy improved from 0.84053 to 0.85084, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 218s - loss: 0.3417 - categorical_accuracy: 0.8508 - top_2_accuracy: 1.0000 - val_loss: 0.3853 - val_categorical_accuracy: 0.8276 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 218s/epoch - 3s/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: categorical_accuracy improved from 0.85084 to 0.85929, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 218s - loss: 0.3258 - categorical_accuracy: 0.8593 - top_2_accuracy: 1.0000 - val_loss: 0.3356 - val_categorical_accuracy: 0.8384 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 218s/epoch - 3s/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: categorical_accuracy did not improve from 0.85929\n",
      "67/67 - 230s - loss: 0.3106 - categorical_accuracy: 0.8546 - top_2_accuracy: 1.0000 - val_loss: 0.3645 - val_categorical_accuracy: 0.8384 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 230s/epoch - 3s/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: categorical_accuracy did not improve from 0.85929\n",
      "67/67 - 304s - loss: 0.3145 - categorical_accuracy: 0.8565 - top_2_accuracy: 1.0000 - val_loss: 0.3438 - val_categorical_accuracy: 0.8491 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 304s/epoch - 5s/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: categorical_accuracy improved from 0.85929 to 0.86773, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 235s - loss: 0.3228 - categorical_accuracy: 0.8677 - top_2_accuracy: 1.0000 - val_loss: 0.3449 - val_categorical_accuracy: 0.8556 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 235s/epoch - 4s/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: categorical_accuracy did not improve from 0.86773\n",
      "67/67 - 233s - loss: 0.3193 - categorical_accuracy: 0.8499 - top_2_accuracy: 1.0000 - val_loss: 0.3202 - val_categorical_accuracy: 0.8513 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 233s/epoch - 3s/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: categorical_accuracy did not improve from 0.86773\n",
      "67/67 - 226s - loss: 0.3145 - categorical_accuracy: 0.8621 - top_2_accuracy: 1.0000 - val_loss: 0.3840 - val_categorical_accuracy: 0.8341 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 226s/epoch - 3s/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: categorical_accuracy did not improve from 0.86773\n",
      "67/67 - 236s - loss: 0.3129 - categorical_accuracy: 0.8649 - top_2_accuracy: 1.0000 - val_loss: 0.3574 - val_categorical_accuracy: 0.8319 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 236s/epoch - 4s/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: categorical_accuracy improved from 0.86773 to 0.87336, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 223s - loss: 0.2876 - categorical_accuracy: 0.8734 - top_2_accuracy: 1.0000 - val_loss: 0.3532 - val_categorical_accuracy: 0.8599 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 223s/epoch - 3s/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: categorical_accuracy did not improve from 0.87336\n",
      "67/67 - 220s - loss: 0.3298 - categorical_accuracy: 0.8659 - top_2_accuracy: 1.0000 - val_loss: 0.3309 - val_categorical_accuracy: 0.8599 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 220s/epoch - 3s/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: categorical_accuracy improved from 0.87336 to 0.88086, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 223s - loss: 0.2911 - categorical_accuracy: 0.8809 - top_2_accuracy: 1.0000 - val_loss: 0.2957 - val_categorical_accuracy: 0.8707 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 223s/epoch - 3s/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: categorical_accuracy improved from 0.88086 to 0.88837, saving model to output/inceptionv3/attention\\pesos_inception_best.h5\n",
      "67/67 - 239s - loss: 0.2666 - categorical_accuracy: 0.8884 - top_2_accuracy: 1.0000 - val_loss: 0.3352 - val_categorical_accuracy: 0.8578 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 239s/epoch - 4s/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 227s - loss: 0.2921 - categorical_accuracy: 0.8649 - top_2_accuracy: 1.0000 - val_loss: 0.3110 - val_categorical_accuracy: 0.8664 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 227s/epoch - 3s/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 226s - loss: 0.2796 - categorical_accuracy: 0.8837 - top_2_accuracy: 1.0000 - val_loss: 0.3114 - val_categorical_accuracy: 0.8707 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 226s/epoch - 3s/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 228s - loss: 0.2682 - categorical_accuracy: 0.8752 - top_2_accuracy: 1.0000 - val_loss: 0.3189 - val_categorical_accuracy: 0.8513 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 228s/epoch - 3s/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: categorical_accuracy did not improve from 0.88837\n",
      "67/67 - 239s - loss: 0.2668 - categorical_accuracy: 0.8837 - top_2_accuracy: 1.0000 - val_loss: 0.3147 - val_categorical_accuracy: 0.8599 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 239s/epoch - 4s/step\n",
      "Epoch 20: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 100). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/inceptionv3/attention/model_inception\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/inceptionv3/attention/model_inception\\assets\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'output/inceptionv3/attention/pesos_inception_best.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='categorical_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')\n",
    "\n",
    "\n",
    "history = model_inception_v3_att.fit(train_generator,\n",
    "                            steps_per_epoch = steps_train,\n",
    "                            epochs = config.epochs,\n",
    "                            callbacks=[history, early_stopping, reduce_lr, checkpoint],\n",
    "                            validation_data = valida_generator,\n",
    "                            validation_steps = steps_val,\n",
    "                            verbose = 2)\n",
    "\n",
    "model_inception_v3_att.save_weights('output/inceptionv3/attention/pesos_inception.h5')\n",
    "model_inception_v3_att.save('output/inceptionv3/attention/model_inception')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODELO INCEPTION V3 SEM ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model loaded\n"
     ]
    }
   ],
   "source": [
    "model_inception_v3_normal = custom_inceptionResnetV3_conv_global()\n",
    "\n",
    "model_inception_v3_normal.compile(optimizer=optimizer_adam,\n",
    "              loss=objective,\n",
    "              metrics=['categorical_accuracy']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: categorical_accuracy improved from -inf to 0.73452, saving model to output/inceptionv3/normal\\pesos_inception_best.h5\n",
      "67/67 - 243s - loss: 0.5628 - categorical_accuracy: 0.7345 - val_loss: 0.4647 - val_categorical_accuracy: 0.7931 - lr: 0.0010 - 243s/epoch - 4s/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: categorical_accuracy improved from 0.73452 to 0.83677, saving model to output/inceptionv3/normal\\pesos_inception_best.h5\n",
      "67/67 - 232s - loss: 0.3938 - categorical_accuracy: 0.8368 - val_loss: 0.4296 - val_categorical_accuracy: 0.8233 - lr: 0.0010 - 232s/epoch - 3s/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: categorical_accuracy improved from 0.83677 to 0.84146, saving model to output/inceptionv3/normal\\pesos_inception_best.h5\n",
      "67/67 - 232s - loss: 0.3513 - categorical_accuracy: 0.8415 - val_loss: 0.4254 - val_categorical_accuracy: 0.8254 - lr: 0.0010 - 232s/epoch - 3s/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: categorical_accuracy improved from 0.84146 to 0.84522, saving model to output/inceptionv3/normal\\pesos_inception_best.h5\n",
      "67/67 - 243s - loss: 0.3589 - categorical_accuracy: 0.8452 - val_loss: 0.3992 - val_categorical_accuracy: 0.8341 - lr: 0.0010 - 243s/epoch - 4s/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: categorical_accuracy did not improve from 0.84522\n",
      "67/67 - 225s - loss: 0.3610 - categorical_accuracy: 0.8452 - val_loss: 0.3711 - val_categorical_accuracy: 0.8405 - lr: 0.0010 - 225s/epoch - 3s/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: categorical_accuracy improved from 0.84522 to 0.86304, saving model to output/inceptionv3/normal\\pesos_inception_best.h5\n",
      "67/67 - 224s - loss: 0.3304 - categorical_accuracy: 0.8630 - val_loss: 0.4049 - val_categorical_accuracy: 0.8491 - lr: 0.0010 - 224s/epoch - 3s/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: categorical_accuracy did not improve from 0.86304\n",
      "67/67 - 223s - loss: 0.3437 - categorical_accuracy: 0.8499 - val_loss: 0.3830 - val_categorical_accuracy: 0.8448 - lr: 0.0010 - 223s/epoch - 3s/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: categorical_accuracy did not improve from 0.86304\n",
      "67/67 - 242s - loss: 0.3359 - categorical_accuracy: 0.8527 - val_loss: 0.4177 - val_categorical_accuracy: 0.8254 - lr: 0.0010 - 242s/epoch - 4s/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: categorical_accuracy did not improve from 0.86304\n",
      "67/67 - 227s - loss: 0.3342 - categorical_accuracy: 0.8621 - val_loss: 0.3707 - val_categorical_accuracy: 0.8427 - lr: 0.0010 - 227s/epoch - 3s/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: categorical_accuracy improved from 0.86304 to 0.87523, saving model to output/inceptionv3/normal\\pesos_inception_best.h5\n",
      "67/67 - 226s - loss: 0.2983 - categorical_accuracy: 0.8752 - val_loss: 0.3858 - val_categorical_accuracy: 0.8276 - lr: 0.0010 - 226s/epoch - 3s/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: categorical_accuracy did not improve from 0.87523\n",
      "67/67 - 223s - loss: 0.3406 - categorical_accuracy: 0.8527 - val_loss: 0.3618 - val_categorical_accuracy: 0.8556 - lr: 0.0010 - 223s/epoch - 3s/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: categorical_accuracy did not improve from 0.87523\n",
      "67/67 - 237s - loss: 0.3118 - categorical_accuracy: 0.8630 - val_loss: 0.4076 - val_categorical_accuracy: 0.8211 - lr: 0.0010 - 237s/epoch - 4s/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: categorical_accuracy did not improve from 0.87523\n",
      "67/67 - 223s - loss: 0.3127 - categorical_accuracy: 0.8602 - val_loss: 0.3660 - val_categorical_accuracy: 0.8513 - lr: 0.0010 - 223s/epoch - 3s/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: categorical_accuracy did not improve from 0.87523\n",
      "67/67 - 224s - loss: 0.2893 - categorical_accuracy: 0.8659 - val_loss: 0.3665 - val_categorical_accuracy: 0.8491 - lr: 0.0010 - 224s/epoch - 3s/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: categorical_accuracy did not improve from 0.87523\n",
      "67/67 - 277s - loss: 0.3079 - categorical_accuracy: 0.8696 - val_loss: 0.3798 - val_categorical_accuracy: 0.8384 - lr: 0.0010 - 277s/epoch - 4s/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: categorical_accuracy did not improve from 0.87523\n",
      "67/67 - 262s - loss: 0.3109 - categorical_accuracy: 0.8677 - val_loss: 0.3827 - val_categorical_accuracy: 0.8384 - lr: 5.0000e-04 - 262s/epoch - 4s/step\n",
      "Epoch 16: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/inceptionv3/normal/model_inception\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/inceptionv3/normal/model_inception\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint_path = 'output/inceptionv3/normal/pesos_inception_best.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='categorical_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')\n",
    "\n",
    "history = model_inception_v3_normal.fit(train_generator,\n",
    "                            steps_per_epoch = steps_train,\n",
    "                            epochs = config.epochs,\n",
    "                            callbacks=[history, early_stopping, reduce_lr, checkpoint],\n",
    "                            validation_data = valida_generator,\n",
    "                            validation_steps = steps_val,\n",
    "                            verbose = 2)\n",
    "\n",
    "model_inception_v3_normal.save_weights('output/inceptionv3/normal/pesos_inception.h5')\n",
    "model_inception_v3_normal.save('output/inceptionv3/normal/model_inception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.5628063678741455,\n",
       "  0.39380645751953125,\n",
       "  0.35126349329948425,\n",
       "  0.35893043875694275,\n",
       "  0.3610437214374542,\n",
       "  0.3304060399532318,\n",
       "  0.3436901867389679,\n",
       "  0.33589157462120056,\n",
       "  0.33416199684143066,\n",
       "  0.29829707741737366,\n",
       "  0.34057098627090454,\n",
       "  0.3118400573730469,\n",
       "  0.3127361834049225,\n",
       "  0.2893204092979431,\n",
       "  0.307861328125,\n",
       "  0.3109379708766937],\n",
       " 'categorical_accuracy': [0.7345215678215027,\n",
       "  0.8367729783058167,\n",
       "  0.8414633870124817,\n",
       "  0.8452157378196716,\n",
       "  0.8452157378196716,\n",
       "  0.8630393743515015,\n",
       "  0.8499062061309814,\n",
       "  0.8527204394340515,\n",
       "  0.8621013164520264,\n",
       "  0.8752345442771912,\n",
       "  0.8527204394340515,\n",
       "  0.8630393743515015,\n",
       "  0.8602251410484314,\n",
       "  0.8658536672592163,\n",
       "  0.8696060180664062,\n",
       "  0.8677298426628113],\n",
       " 'val_loss': [0.46467846632003784,\n",
       "  0.4296267330646515,\n",
       "  0.42544519901275635,\n",
       "  0.39915329217910767,\n",
       "  0.3711182475090027,\n",
       "  0.4049389064311981,\n",
       "  0.38298866152763367,\n",
       "  0.4177466034889221,\n",
       "  0.37070199847221375,\n",
       "  0.3858361840248108,\n",
       "  0.361804336309433,\n",
       "  0.40756988525390625,\n",
       "  0.36604899168014526,\n",
       "  0.36645621061325073,\n",
       "  0.379810631275177,\n",
       "  0.38271069526672363],\n",
       " 'val_categorical_accuracy': [0.7931034564971924,\n",
       "  0.8232758641242981,\n",
       "  0.8254310488700867,\n",
       "  0.8340517282485962,\n",
       "  0.8405172228813171,\n",
       "  0.8491379022598267,\n",
       "  0.8448275923728943,\n",
       "  0.8254310488700867,\n",
       "  0.8426724076271057,\n",
       "  0.8275862336158752,\n",
       "  0.8556034564971924,\n",
       "  0.8211206793785095,\n",
       "  0.8512930870056152,\n",
       "  0.8491379022598267,\n",
       "  0.8383620977401733,\n",
       "  0.8383620977401733],\n",
       " 'lr': [0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.0005]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./output/inceptionv3/normal/history_v3_normal.json', 'w') as h:\n",
    "    json.dump(str(history.history), h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model loaded\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)           [(None, 222, 222, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " inception_resnet_v2 (Functiona  (None, 5, 5, 1536)  54336736    ['input_8[0][0]']                \n",
      " l)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization_703 (Batch  (None, 5, 5, 1536)  6144        ['inception_resnet_v2[1][0]']    \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 5, 5, 1536)   0           ['batch_normalization_703[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_703 (Conv2D)            (None, 5, 5, 64)     98368       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_704 (Conv2D)            (None, 5, 5, 16)     1040        ['conv2d_703[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_705 (Conv2D)            (None, 5, 5, 8)      136         ['conv2d_704[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_706 (Conv2D)            (None, 5, 5, 1)      9           ['conv2d_705[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_707 (Conv2D)            (None, 5, 5, 1536)   1536        ['conv2d_706[0][0]']             \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 5, 5, 1536)   0           ['conv2d_707[0][0]',             \n",
      "                                                                  'batch_normalization_703[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1536)        0           ['multiply[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1536)        0           ['conv2d_707[0][0]']             \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      " RescaleGAP (Lambda)            (None, 1536)         0           ['global_average_pooling2d[0][0]'\n",
      "                                                                 , 'global_average_pooling2d_1[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1536)         0           ['RescaleGAP[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          196736      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 2)            258         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,640,963\n",
      "Trainable params: 299,619\n",
      "Non-trainable params: 54,341,344\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelo_inception_v2_attention = custom_inceptionV2_attention()\n",
    "\n",
    "checkpoint_path = 'output/inceptionv2/attention/pesos_inception_best.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='categorical_accuracy', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\n",
      "Epoch 1: categorical_accuracy improved from -inf to 0.78424, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 376s - loss: 0.4968 - categorical_accuracy: 0.7842 - top_2_accuracy: 1.0000 - val_loss: 0.4708 - val_categorical_accuracy: 0.7909 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 376s/epoch - 6s/step\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 2: categorical_accuracy improved from 0.78424 to 0.81707, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 351s - loss: 0.3936 - categorical_accuracy: 0.8171 - top_2_accuracy: 1.0000 - val_loss: 0.5300 - val_categorical_accuracy: 0.7737 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 351s/epoch - 5s/step\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 3: categorical_accuracy improved from 0.81707 to 0.84428, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 359s - loss: 0.3342 - categorical_accuracy: 0.8443 - top_2_accuracy: 1.0000 - val_loss: 0.3637 - val_categorical_accuracy: 0.8556 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 359s/epoch - 5s/step\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 4: categorical_accuracy improved from 0.84428 to 0.85835, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 340s - loss: 0.3241 - categorical_accuracy: 0.8583 - top_2_accuracy: 1.0000 - val_loss: 0.3513 - val_categorical_accuracy: 0.8815 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 340s/epoch - 5s/step\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 5: categorical_accuracy did not improve from 0.85835\n",
      "67/67 - 351s - loss: 0.3522 - categorical_accuracy: 0.8537 - top_2_accuracy: 1.0000 - val_loss: 0.3696 - val_categorical_accuracy: 0.8341 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 351s/epoch - 5s/step\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 6: categorical_accuracy did not improve from 0.85835\n",
      "67/67 - 362s - loss: 0.3302 - categorical_accuracy: 0.8499 - top_2_accuracy: 1.0000 - val_loss: 0.2926 - val_categorical_accuracy: 0.8642 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 362s/epoch - 5s/step\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 7: categorical_accuracy improved from 0.85835 to 0.86023, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 335s - loss: 0.3147 - categorical_accuracy: 0.8602 - top_2_accuracy: 1.0000 - val_loss: 0.3509 - val_categorical_accuracy: 0.8599 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 335s/epoch - 5s/step\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 8: categorical_accuracy improved from 0.86023 to 0.86398, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 352s - loss: 0.2957 - categorical_accuracy: 0.8640 - top_2_accuracy: 1.0000 - val_loss: 0.2957 - val_categorical_accuracy: 0.8664 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 352s/epoch - 5s/step\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 9: categorical_accuracy improved from 0.86398 to 0.87148, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 335s - loss: 0.2788 - categorical_accuracy: 0.8715 - top_2_accuracy: 1.0000 - val_loss: 0.3775 - val_categorical_accuracy: 0.8534 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 335s/epoch - 5s/step\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 10: categorical_accuracy did not improve from 0.87148\n",
      "67/67 - 336s - loss: 0.2997 - categorical_accuracy: 0.8668 - top_2_accuracy: 1.0000 - val_loss: 0.2999 - val_categorical_accuracy: 0.8772 - val_top_2_accuracy: 1.0000 - lr: 0.0010 - 336s/epoch - 5s/step\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 11: categorical_accuracy improved from 0.87148 to 0.88368, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 353s - loss: 0.2754 - categorical_accuracy: 0.8837 - top_2_accuracy: 1.0000 - val_loss: 0.2852 - val_categorical_accuracy: 0.8685 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 353s/epoch - 5s/step\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 12: categorical_accuracy did not improve from 0.88368\n",
      "67/67 - 337s - loss: 0.2709 - categorical_accuracy: 0.8762 - top_2_accuracy: 1.0000 - val_loss: 0.3103 - val_categorical_accuracy: 0.8621 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 337s/epoch - 5s/step\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 13: categorical_accuracy did not improve from 0.88368\n",
      "67/67 - 336s - loss: 0.2492 - categorical_accuracy: 0.8780 - top_2_accuracy: 1.0000 - val_loss: 0.3036 - val_categorical_accuracy: 0.8707 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 336s/epoch - 5s/step\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 14: categorical_accuracy did not improve from 0.88368\n",
      "67/67 - 345s - loss: 0.3025 - categorical_accuracy: 0.8677 - top_2_accuracy: 1.0000 - val_loss: 0.3329 - val_categorical_accuracy: 0.8341 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 345s/epoch - 5s/step\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 15: categorical_accuracy did not improve from 0.88368\n",
      "67/67 - 334s - loss: 0.2711 - categorical_accuracy: 0.8771 - top_2_accuracy: 1.0000 - val_loss: 0.2989 - val_categorical_accuracy: 0.8621 - val_top_2_accuracy: 1.0000 - lr: 5.0000e-04 - 334s/epoch - 5s/step\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 16: categorical_accuracy improved from 0.88368 to 0.88743, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 356s - loss: 0.2358 - categorical_accuracy: 0.8874 - top_2_accuracy: 1.0000 - val_loss: 0.2983 - val_categorical_accuracy: 0.8685 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 356s/epoch - 5s/step\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 17: categorical_accuracy improved from 0.88743 to 0.89306, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 340s - loss: 0.2477 - categorical_accuracy: 0.8931 - top_2_accuracy: 1.0000 - val_loss: 0.2753 - val_categorical_accuracy: 0.8815 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 340s/epoch - 5s/step\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 18: categorical_accuracy did not improve from 0.89306\n",
      "67/67 - 336s - loss: 0.2616 - categorical_accuracy: 0.8865 - top_2_accuracy: 1.0000 - val_loss: 0.3249 - val_categorical_accuracy: 0.8621 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 336s/epoch - 5s/step\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 19: categorical_accuracy did not improve from 0.89306\n",
      "67/67 - 350s - loss: 0.2420 - categorical_accuracy: 0.8921 - top_2_accuracy: 1.0000 - val_loss: 0.2802 - val_categorical_accuracy: 0.8664 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 350s/epoch - 5s/step\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 20: categorical_accuracy did not improve from 0.89306\n",
      "67/67 - 337s - loss: 0.2410 - categorical_accuracy: 0.8827 - top_2_accuracy: 1.0000 - val_loss: 0.2814 - val_categorical_accuracy: 0.8793 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 337s/epoch - 5s/step\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 21: categorical_accuracy did not improve from 0.89306\n",
      "67/67 - 354s - loss: 0.2598 - categorical_accuracy: 0.8893 - top_2_accuracy: 1.0000 - val_loss: 0.2608 - val_categorical_accuracy: 0.8750 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 354s/epoch - 5s/step\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 22: categorical_accuracy did not improve from 0.89306\n",
      "67/67 - 337s - loss: 0.2363 - categorical_accuracy: 0.8912 - top_2_accuracy: 1.0000 - val_loss: 0.2662 - val_categorical_accuracy: 0.8901 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 337s/epoch - 5s/step\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 23: categorical_accuracy improved from 0.89306 to 0.89400, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 337s - loss: 0.2383 - categorical_accuracy: 0.8940 - top_2_accuracy: 1.0000 - val_loss: 0.3210 - val_categorical_accuracy: 0.8793 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 337s/epoch - 5s/step\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 24: categorical_accuracy did not improve from 0.89400\n",
      "67/67 - 354s - loss: 0.2620 - categorical_accuracy: 0.8865 - top_2_accuracy: 1.0000 - val_loss: 0.2922 - val_categorical_accuracy: 0.8664 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 354s/epoch - 5s/step\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 25: categorical_accuracy did not improve from 0.89400\n",
      "67/67 - 370s - loss: 0.2449 - categorical_accuracy: 0.8856 - top_2_accuracy: 1.0000 - val_loss: 0.2683 - val_categorical_accuracy: 0.8836 - val_top_2_accuracy: 1.0000 - lr: 2.5000e-04 - 370s/epoch - 6s/step\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 26: categorical_accuracy did not improve from 0.89400\n",
      "67/67 - 437s - loss: 0.2423 - categorical_accuracy: 0.8884 - top_2_accuracy: 1.0000 - val_loss: 0.2908 - val_categorical_accuracy: 0.8685 - val_top_2_accuracy: 1.0000 - lr: 1.2500e-04 - 437s/epoch - 7s/step\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 27: categorical_accuracy improved from 0.89400 to 0.90432, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 385s - loss: 0.2313 - categorical_accuracy: 0.9043 - top_2_accuracy: 1.0000 - val_loss: 0.3033 - val_categorical_accuracy: 0.8556 - val_top_2_accuracy: 1.0000 - lr: 1.2500e-04 - 385s/epoch - 6s/step\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 28: categorical_accuracy improved from 0.90432 to 0.90525, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 375s - loss: 0.2257 - categorical_accuracy: 0.9053 - top_2_accuracy: 1.0000 - val_loss: 0.3139 - val_categorical_accuracy: 0.8599 - val_top_2_accuracy: 1.0000 - lr: 1.2500e-04 - 375s/epoch - 6s/step\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 29: categorical_accuracy did not improve from 0.90525\n",
      "67/67 - 431s - loss: 0.2421 - categorical_accuracy: 0.8940 - top_2_accuracy: 1.0000 - val_loss: 0.2949 - val_categorical_accuracy: 0.8772 - val_top_2_accuracy: 1.0000 - lr: 1.2500e-04 - 431s/epoch - 6s/step\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 30: categorical_accuracy did not improve from 0.90525\n",
      "67/67 - 389s - loss: 0.2174 - categorical_accuracy: 0.9024 - top_2_accuracy: 1.0000 - val_loss: 0.2761 - val_categorical_accuracy: 0.8728 - val_top_2_accuracy: 1.0000 - lr: 6.2500e-05 - 389s/epoch - 6s/step\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 31: categorical_accuracy did not improve from 0.90525\n",
      "67/67 - 386s - loss: 0.2389 - categorical_accuracy: 0.8968 - top_2_accuracy: 1.0000 - val_loss: 0.3094 - val_categorical_accuracy: 0.8556 - val_top_2_accuracy: 1.0000 - lr: 6.2500e-05 - 386s/epoch - 6s/step\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 32: categorical_accuracy did not improve from 0.90525\n",
      "67/67 - 367s - loss: 0.2449 - categorical_accuracy: 0.8902 - top_2_accuracy: 1.0000 - val_loss: 0.2861 - val_categorical_accuracy: 0.8599 - val_top_2_accuracy: 1.0000 - lr: 6.2500e-05 - 367s/epoch - 5s/step\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 33: categorical_accuracy improved from 0.90525 to 0.90807, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 398s - loss: 0.2240 - categorical_accuracy: 0.9081 - top_2_accuracy: 1.0000 - val_loss: 0.2671 - val_categorical_accuracy: 0.8728 - val_top_2_accuracy: 1.0000 - lr: 6.2500e-05 - 398s/epoch - 6s/step\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 34: categorical_accuracy did not improve from 0.90807\n",
      "67/67 - 372s - loss: 0.2105 - categorical_accuracy: 0.9081 - top_2_accuracy: 1.0000 - val_loss: 0.3019 - val_categorical_accuracy: 0.8966 - val_top_2_accuracy: 1.0000 - lr: 3.1250e-05 - 372s/epoch - 6s/step\n",
      "Epoch 35/50\n",
      "\n",
      "Epoch 35: categorical_accuracy did not improve from 0.90807\n",
      "67/67 - 376s - loss: 0.2222 - categorical_accuracy: 0.9071 - top_2_accuracy: 1.0000 - val_loss: 0.2715 - val_categorical_accuracy: 0.8664 - val_top_2_accuracy: 1.0000 - lr: 3.1250e-05 - 376s/epoch - 6s/step\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 36: categorical_accuracy did not improve from 0.90807\n",
      "67/67 - 396s - loss: 0.2294 - categorical_accuracy: 0.9053 - top_2_accuracy: 1.0000 - val_loss: 0.2870 - val_categorical_accuracy: 0.8836 - val_top_2_accuracy: 1.0000 - lr: 3.1250e-05 - 396s/epoch - 6s/step\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 37: categorical_accuracy did not improve from 0.90807\n",
      "67/67 - 350s - loss: 0.2317 - categorical_accuracy: 0.9043 - top_2_accuracy: 1.0000 - val_loss: 0.2689 - val_categorical_accuracy: 0.8707 - val_top_2_accuracy: 1.0000 - lr: 3.1250e-05 - 350s/epoch - 5s/step\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 38: categorical_accuracy did not improve from 0.90807\n",
      "67/67 - 393s - loss: 0.2239 - categorical_accuracy: 0.9043 - top_2_accuracy: 1.0000 - val_loss: 0.2744 - val_categorical_accuracy: 0.8944 - val_top_2_accuracy: 1.0000 - lr: 1.5625e-05 - 393s/epoch - 6s/step\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 39: categorical_accuracy improved from 0.90807 to 0.90901, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 396s - loss: 0.2143 - categorical_accuracy: 0.9090 - top_2_accuracy: 1.0000 - val_loss: 0.2695 - val_categorical_accuracy: 0.8707 - val_top_2_accuracy: 1.0000 - lr: 1.5625e-05 - 396s/epoch - 6s/step\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 40: categorical_accuracy did not improve from 0.90901\n",
      "67/67 - 404s - loss: 0.2365 - categorical_accuracy: 0.8987 - top_2_accuracy: 1.0000 - val_loss: 0.2746 - val_categorical_accuracy: 0.8793 - val_top_2_accuracy: 1.0000 - lr: 1.5625e-05 - 404s/epoch - 6s/step\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 41: categorical_accuracy improved from 0.90901 to 0.91088, saving model to output/inceptionv2/attention\\pesos_inception_best.h5\n",
      "67/67 - 392s - loss: 0.2107 - categorical_accuracy: 0.9109 - top_2_accuracy: 1.0000 - val_loss: 0.2617 - val_categorical_accuracy: 0.8858 - val_top_2_accuracy: 1.0000 - lr: 1.5625e-05 - 392s/epoch - 6s/step\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 42: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 372s - loss: 0.2398 - categorical_accuracy: 0.8921 - top_2_accuracy: 1.0000 - val_loss: 0.2719 - val_categorical_accuracy: 0.8772 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 372s/epoch - 6s/step\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 43: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 389s - loss: 0.2005 - categorical_accuracy: 0.9071 - top_2_accuracy: 1.0000 - val_loss: 0.2626 - val_categorical_accuracy: 0.8793 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 389s/epoch - 6s/step\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 44: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 331s - loss: 0.2441 - categorical_accuracy: 0.8931 - top_2_accuracy: 1.0000 - val_loss: 0.3050 - val_categorical_accuracy: 0.8621 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 331s/epoch - 5s/step\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 45: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 346s - loss: 0.2070 - categorical_accuracy: 0.9081 - top_2_accuracy: 1.0000 - val_loss: 0.2871 - val_categorical_accuracy: 0.8685 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 346s/epoch - 5s/step\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 46: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 345s - loss: 0.2161 - categorical_accuracy: 0.9090 - top_2_accuracy: 1.0000 - val_loss: 0.2944 - val_categorical_accuracy: 0.8599 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 345s/epoch - 5s/step\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 47: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 348s - loss: 0.2106 - categorical_accuracy: 0.9081 - top_2_accuracy: 1.0000 - val_loss: 0.3103 - val_categorical_accuracy: 0.8685 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 348s/epoch - 5s/step\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 48: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 369s - loss: 0.2497 - categorical_accuracy: 0.8912 - top_2_accuracy: 1.0000 - val_loss: 0.2938 - val_categorical_accuracy: 0.8707 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 369s/epoch - 6s/step\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 49: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 336s - loss: 0.2405 - categorical_accuracy: 0.8818 - top_2_accuracy: 1.0000 - val_loss: 0.2764 - val_categorical_accuracy: 0.8772 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 336s/epoch - 5s/step\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 50: categorical_accuracy did not improve from 0.91088\n",
      "67/67 - 355s - loss: 0.2106 - categorical_accuracy: 0.9015 - top_2_accuracy: 1.0000 - val_loss: 0.2515 - val_categorical_accuracy: 0.8858 - val_top_2_accuracy: 1.0000 - lr: 1.0000e-05 - 355s/epoch - 5s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 250). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/inceptionv2/attention/model_inception\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: output/inceptionv2/attention/model_inception\\assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = modelo_inception_v2_attention.fit(train_generator,\n",
    "                            steps_per_epoch = steps_train,\n",
    "                            epochs = config.epochs,\n",
    "                            callbacks=[history, reduce_lr, checkpoint],\n",
    "                            validation_data = valida_generator,\n",
    "                            validation_steps = steps_val,\n",
    "                            verbose = 2)\n",
    "\n",
    "modelo_inception_v2_attention.save_weights('output/inceptionv2/attention/pesos_inception.h5')\n",
    "modelo_inception_v2_attention.save('output/inceptionv2/attention/model_inception')\n",
    "\n",
    "with open('./output/inceptionv2/attention/history_v2_attention.json', 'w') as h:\n",
    "    json.dump(str(history.history), h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_inception_v2_attention_t = modelo_inception_v2_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_inception_v2_attention_t.load_weights('output/inceptionv2/attention/pesos_inception_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image = utils.load('./datasets/VALIDACAO/images/training/IDRiD_197.jpg')\n",
    "model.evaluate(valida_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model\n",
    "history = model.fit(train_generator,\n",
    "                            steps_per_epoch = steps_train,\n",
    "                            epochs = 5, #config.epochs,\n",
    "                            callbacks=[history, early_stopping, reduce_lr],\n",
    "                            validation_data = valida_generator,\n",
    "                            validation_steps = steps_val,\n",
    "                            verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lp_eyenet_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7818a22cc365f8045c7656bbb6e4a0740739cc05f4afd70a528805e6f7c69cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
